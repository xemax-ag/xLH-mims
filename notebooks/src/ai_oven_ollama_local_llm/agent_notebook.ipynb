{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be97dee-4c17-4539-afcf-2eb4d5bd12a0",
   "metadata": {},
   "source": [
    "# xLH-ai-oven\n",
    "Eine exemplarische Umsetzung für die Integration von LLM-Modellen an Physikalische Umgebungen (IoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4342e5-d5e5-46b0-805d-26950a66d1f8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/GitHub/xLH-mims/notebooks/src/ai_oven_ollama_local_llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "__cwd__ = str(pathlib.Path(os.getcwd())).replace('\\\\', '/')\n",
    "sys.path.append(__cwd__)\n",
    "print(__cwd__)\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bb2d5d-ef82-4d57-ae83-8d7da471bddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\xLH-mims-data\\config\\xlh_mims_python.env\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic_ai import Tool\n",
    "from rich import print\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent\n",
    "from tools import set_oven_temperature\n",
    "from llm_model import get_model, LlmModel\n",
    "from object_to_file import base_model_to_file\n",
    "from config import config\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fb6af-cbf1-4251-bf29-8fbf4cb7a0b3",
   "metadata": {},
   "source": [
    "![Ollama Settings](Ollama_Settings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d83efdd-4a46-4947-99e8-a197243b1ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Deps:\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d48895-4b56-45d4-aaf2-2b8252fe4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ingredient(BaseModel):\n",
    "    name: str = Field(description='Name der Zutat')\n",
    "    quantity: str = Field(description='Menge')\n",
    "    unit: str = Field(description='Einheit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c1738e-a86d-4b1f-801d-9e841acb5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe(BaseModel):\n",
    "    title: str = Field(description='Name der Pizza')\n",
    "    ingredients: list[Ingredient] = Field(description='Zutaten für die Menuezubereitung')\n",
    "    preparation_steps: list[str] = Field(description='Zubereitungsschritte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42736dfce46ee2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(llm_model=LlmModel.OLLAMA_GPT_OSS_20B)  # ToDo: Integration OLLAMA...\n",
    "agent= Agent(\n",
    "    model=model,\n",
    "    system_prompt=('Du bist ein Pizzabäcker welcher Rezepte für kreative Pizzas kreirt. '\n",
    "                   'Nutze das Tool set_oven_temperature für die Einstellung des Backofens.'),\n",
    "    deps_type=Deps,\n",
    "    tools=[\n",
    "        Tool(set_oven_temperature, takes_ctx=True),\n",
    "    ],\n",
    "    retries=3,\n",
    "    output_type=Recipe,\n",
    ")\n",
    "deps = Deps(name='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847347c9-df2a-4432-93f5-9fd045918b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> tool_set_oven_temperature 225.0\n",
      "=> set_oven_temperature_opcua 225.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_output.py\", line 625, in process\n",
      "    output = self.validate(data, allow_partial=allow_partial, validation_context=run_context.validation_context)\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_output.py\", line 648, in validate\n",
      "    return self.validator.validate_json(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        data or '{}', allow_partial=pyd_allow_partial, context=validation_context\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "pydantic_core._pydantic_core.ValidationError: 9 validation errors for Recipe\n",
      "preparation_steps.0\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 1, 'description'...25\\u202f°C vorheizen.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "preparation_steps.1\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 2, 'description'... auf Backpapier legen.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "preparation_steps.2\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 3, 'description'...d Basilikum bestreuen.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "preparation_steps.3\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 4, 'description'...uf dem Teig verteilen.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "preparation_steps.4\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 5, 'description'...eiben darauf anordnen.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "preparation_steps.5\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 6, 'description'... und darüber streuen.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "preparation_steps.6\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 7, 'description'...corino darüber geben.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "preparation_steps.7\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 8, 'description'...er Rand goldbraun ist.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "preparation_steps.8\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 9, 'description'...rnieren und servieren.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 729, in _run_stream\n",
      "    self._next_node = await self._handle_text_response(ctx, text, text_processor)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 806, in _handle_text_response\n",
      "    result_data = await text_processor.process(text, run_context=run_context)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_output.py\", line 631, in process\n",
      "    raise ToolRetryError(m) from e\n",
      "pydantic_ai.exceptions.ToolRetryError: 9 validation errors\n",
      "preparation_steps.0\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 1, 'description': 'Backofen auf 225\\u202f°C vorheizen.'}]\n",
      "preparation_steps.1\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 2, 'description': 'Pizzateig auf einer bemehlten Fläche ausrollen und auf Backpapier legen.'}]\n",
      "preparation_steps.2\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 3, 'description': 'Teig mit Olivenöl beträufeln, Salz, Pfeffer, Oregano und Basilikum bestreuen.'}]\n",
      "preparation_steps.3\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 4, 'description': 'Tomatenscheiben gleichmäßig auf dem Teig verteilen.'}]\n",
      "preparation_steps.4\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 5, 'description': 'Salami‑Streifen und Lachs‑Scheiben darauf anordnen.'}]\n",
      "preparation_steps.5\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 6, 'description': 'Knoblauch fein hacken und darüber streuen.'}]\n",
      "preparation_steps.6\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 7, 'description': 'Optional Parmesan/Pecorino darüber geben.'}]\n",
      "preparation_steps.7\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 8, 'description': 'Pizza 20‑25\\u202fMinuten backen, bis der Rand goldbraun ist.'}]\n",
      "preparation_steps.8\n",
      "  Input should be a valid string [type=string_type, input_value={'step': 9, 'description': 'Aus dem Ofen nehmen, kurz abkühlen lassen, mit Rucola/Spinat garnieren und servieren.'}]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\phe\\AppData\\Local\\Temp\\ipykernel_34672\\718736782.py\", line 1, in <module>\n",
      "    response = agent.run_sync(user_prompt='Ich habe im Kühlschrank Lachs, Salami und Tomaten '\n",
      "               ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                     'Kreire mir eine Pizza. '\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                     'Da mein Backofen etwas alterschwach ist, '\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                     'kann die Temperatur nicht höher als 225 Grad eingestellt werden '\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                     'und die Backzeit darf 30 Minuten nicht überschreiten.')\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\abstract.py\", line 386, in run_sync\n",
      "    return _utils.get_event_loop().run_until_complete(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self.run(\n",
      "        ^^^^^^^^^\n",
      "    ...<15 lines>...\n",
      "        )\n",
      "        ^\n",
      "    )\n",
      "    ^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ~~~~~~~~^^\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\asyncio\\futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\asyncio\\tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\abstract.py\", line 259, in run\n",
      "    async with self.iter(\n",
      "               ~~~~~~~~~^\n",
      "        user_prompt=user_prompt,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "        builtin_tools=builtin_tools,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ) as agent_run:\n",
      "    ^\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\contextlib.py\", line 235, in __aexit__\n",
      "    await self.gen.athrow(value)\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\__init__.py\", line 722, in iter\n",
      "    graph.iter(\n",
      "    ~~~~~~~~~~^\n",
      "        inputs=user_prompt_node,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        infer_name=False,\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "    ) as graph_run,\n",
      "    ^\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\contextlib.py\", line 235, in __aexit__\n",
      "    await self.gen.athrow(value)\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 270, in iter\n",
      "    async with GraphRun[StateT, DepsT, OutputT](\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        graph=self,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        traceparent=traceparent,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ) as graph_run:\n",
      "    ^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 423, in __aexit__\n",
      "    await self._async_exit_stack.__aexit__(exc_type, exc_val, exc_tb)\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\contextlib.py\", line 768, in __aexit__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\contextlib.py\", line 749, in __aexit__\n",
      "    cb_suppress = cb(*exc_details)\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\contextlib.py\", line 162, in __exit__\n",
      "    self.gen.throw(value)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 981, in _unwrap_exception_groups\n",
      "    raise exception\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\asyncio\\tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 750, in _run_tracked_task\n",
      "    result = await self._run_task(t_)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\graph.py\", line 782, in _run_task\n",
      "    output = await node.call(step_context)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_graph\\beta\\step.py\", line 253, in _call_node\n",
      "    return await node.run(GraphRunContext(state=ctx.state, deps=ctx.deps))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 593, in run\n",
      "    async with self.stream(ctx):\n",
      "               ~~~~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\phe\\AppData\\Roaming\\uv\\python\\cpython-3.13.7-windows-x86_64-none\\Lib\\contextlib.py\", line 221, in __aexit__\n",
      "    await anext(self.gen)\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 607, in stream\n",
      "    async for _event in stream:\n",
      "        pass\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 752, in _run_stream\n",
      "    async for event in self._events_iterator:\n",
      "        yield event\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 741, in _run_stream\n",
      "    ctx.state.increment_retries(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        ctx.deps.max_result_retries, error=e, model_settings=ctx.deps.model_settings\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"D:\\GitHub\\xLH-mims\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py\", line 122, in increment_retries\n",
      "    raise exceptions.UnexpectedModelBehavior(message) from error\n",
      "pydantic_ai.exceptions.UnexpectedModelBehavior: Exceeded maximum retries (3) for output validation\n"
     ]
    }
   ],
   "source": [
    "response = agent.run_sync(user_prompt='Ich habe im Kühlschrank Lachs, Salami und Tomaten '\n",
    "                                 'Kreire mir eine Pizza. '\n",
    "                                 'Da mein Backofen etwas alterschwach ist, '\n",
    "                                 'kann die Temperatur nicht höher als 225 Grad eingestellt werden '\n",
    "                                 'und die Backzeit darf 30 Minuten nicht überschreiten.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48fc20f0e1c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "result: Recipe = response.output\n",
    "base_model_to_file(result)\n",
    "# siehe recipe.json\n",
    "print(f'Antwort: {result.model_dump_json(indent=4)[:100]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea04e0cdaf3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage = response.usage()\n",
    "print(f'Input Tokens: {usage.input_tokens} ({usage.input_tokens*1.75*1e-6:0.04f} $)')\n",
    "print(f'Output Tokens: {usage.output_tokens} ({usage.output_tokens*14.0*1e-6:0.04f} $)')\n",
    "print(f'Total Tokens: {usage.total_tokens} (total cost: {(usage.input_tokens*1.75*1e-6 + usage.input_tokens*1.75*1e-6):0.04f} $)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1d640f58cb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = agent.model.settings\n",
    "print(f'Settings: {settings}')\n",
    "print(f'temperature: {settings.get('temperature')}')\n",
    "print(f'presence_penalty: {settings.get('presence_penalty')}')\n",
    "print(f'frequency_penalty: {settings.get('frequency_penalty')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5f251-328a-4983-913c-cb59cc64bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
